{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "yx3HaKsl4tt3",
      "metadata": {
        "id": "yx3HaKsl4tt3"
      },
      "source": [
        "\n",
        "#### Roll no 102103430\n",
        "Rimjhim Mittal\n",
        "3CO16\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bc60800",
      "metadata": {
        "id": "0bc60800"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0fa07f0",
      "metadata": {
        "id": "e0fa07f0"
      },
      "source": [
        "#### Q1. K-Fold Cross Validation for Multiple Linear Regression (Least Square Error Fit)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "700e8cc4",
      "metadata": {
        "id": "700e8cc4"
      },
      "source": [
        "Implement a 5-fold cross-validation technique for multiple linear regression using the least square error method on the USA House Price dataset.\n",
        "\n",
        "**Dataset link** https://drive.google.com/file/d/1O_NwpJT-8xGfU_-3llUl2sgPu0xllOrX/view\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. **Data Preparation:**\n",
        "    * Load the USA House Price dataset.\n",
        "    * Segment the dataset into predictor variables (all columns excluding 'Price') and the target variable ('Price').\n",
        "\n",
        "2. **Feature Scaling:**\n",
        "    * Scale the predictor variables to ensure they are on the same scale. This can be achieved using techniques such as MinMax scaling or Standard scaling.\n",
        "\n",
        "3. **5-Fold Cross-Validation Setup:**\n",
        "    * Partition the predictor variables and the target variable into five equal subsets or \"folds\".\n",
        "\n",
        "4. **Model Training and Evaluation:**\n",
        "    * For each iteration (total of 5):\n",
        "        - Treat one fold as the validation set and the remaining four folds as the training set.\n",
        "        - Compute the beta (\\( \\beta \\)) matrix using the least square error fit method.\n",
        "        - Predict the output for the validation set using the computed \\( \\beta \\) matrix.\n",
        "        - Evaluate the performance of the model using the \\( R^2 \\) score for the validation set.\n",
        "    * Keep track of the \\( \\beta \\) matrix that yields the highest \\( R^2 \\) score.\n",
        "\n",
        "5. **Final Model Assessment:**\n",
        "    * Use the optimal \\( \\beta \\) matrix (corresponding to the highest \\( R^2 \\) score) from the cross-validation phase.\n",
        "    * Train the regressor on 70% of the entire dataset.\n",
        "    * Test the regressor's performance on the remaining 30% of the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pQfjw0bP4stt",
      "metadata": {
        "id": "pQfjw0bP4stt"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "798af5b0",
      "metadata": {
        "id": "798af5b0"
      },
      "outputs": [],
      "source": [
        "def load_data(filename):\n",
        "    return pd.read_csv(filename)\n",
        "\n",
        "def normalize_data(df):\n",
        "    scaler = MinMaxScaler()\n",
        "    return pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3e961a5",
      "metadata": {
        "id": "d3e961a5"
      },
      "outputs": [],
      "source": [
        "def train_sklearn_regression(x_train, y_train, x_test, y_test):\n",
        "    reg = LinearRegression()\n",
        "    reg.fit(x_train, y_train)\n",
        "    predictions = reg.predict(x_test)\n",
        "    return reg.coef_, r2_score(y_test, predictions), mean_absolute_error(y_test, predictions)\n",
        "def gradient_descent_regression(X, Y, learning_rate, iterations):\n",
        "    m, n = X.shape\n",
        "    theta = np.zeros((n, 1))\n",
        "    for _ in range(iterations):\n",
        "        predictions = X @ theta\n",
        "        errors = predictions - Y\n",
        "        gradients = (1/m) * X.T @ errors\n",
        "        theta -= learning_rate * gradients\n",
        "    return theta\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3ead3c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3ead3c1",
        "outputId": "77a870ca-586e-4217-e824-39ca79868d7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sklearn Regression R2 Score: 0.9147262743223825\n",
            "Sklearn Regression Coefficients: [0.78972693 0.46379384 0.36821131 0.00512161 0.43001234]\n",
            "Sklearn Regression MAE: 0.03307267725934341\n",
            "Gradient Descent Coefficients: [[0.16299955]\n",
            " [0.1839815 ]\n",
            " [0.14779516]\n",
            " [0.11516578]\n",
            " [0.06516494]\n",
            " [0.14645295]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def main():\n",
        "    # Load and preprocess the data\n",
        "    df = load_data('./USA_Housing.csv')\n",
        "    df = normalize_data(df)\n",
        "\n",
        "    df_train_validate, df_test = train_test_split(df, test_size=0.3, random_state=42)\n",
        "    df_train, df_validate = train_test_split(df_train_validate, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "    # Separate features and target variable\n",
        "    features = ['Avg. Area Income','Avg. Area House Age','Avg. Area Number of Rooms',\n",
        "                'Avg. Area Number of Bedrooms','Area Population']\n",
        "    x_train = df_train[features]\n",
        "    y_train = df_train['Price']\n",
        "    x_validate = df_validate[features]\n",
        "    y_validate = df_validate['Price']\n",
        "    x_test = df_test[features]\n",
        "    y_test = df_test['Price']\n",
        "\n",
        "    # Train the model using sklearn\n",
        "    coef, r2, mae = train_sklearn_regression(x_train, y_train, x_test, y_test)\n",
        "    print(f\"Sklearn Regression R2 Score: {r2}\")\n",
        "    print(f\"Sklearn Regression Coefficients: {coef}\")\n",
        "    print(f\"Sklearn Regression MAE: {mae}\")\n",
        "\n",
        "    # Train the model using gradient descent\n",
        "    X = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
        "    theta = gradient_descent_regression(X, y_train.values.reshape(-1, 1), 0.01, 1000)\n",
        "    print(f\"Gradient Descent Coefficients: {theta}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e794cd5",
      "metadata": {
        "id": "4e794cd5"
      },
      "source": [
        "#### Q2. Concept of Validation set for Multiple Linear Regression (Gradient Descent Optimization)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5320c9d6",
      "metadata": {
        "id": "5320c9d6"
      },
      "source": [
        "\n",
        "Evaluate the performance of multiple linear regression using gradient descent with various learning rates on the USA House Price dataset.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. **Data Preparation:**\n",
        "    * Load the USA House Price dataset.\n",
        "    * Partition the dataset into:\n",
        "        - Training set (56%)\n",
        "        - Validation set (14%)\n",
        "        - Test set (30%)\n",
        "\n",
        "2. **Model Training:**\n",
        "    * Consider the following learning rates: {0.001, 0.01, 0.1, 1}.\n",
        "    * For each learning rate:\n",
        "        - Train the multiple linear regression model using gradient descent for 1000 iterations.\n",
        "        - Record the regression coefficients (\\( \\beta \\) values) after the iterations.\n",
        "\n",
        "3. **Model Evaluation:**\n",
        "    * For each set of regression coefficients obtained from different learning rates:\n",
        "        - Predict the outcomes for the validation set and compute the \\( R^2 \\) score.\n",
        "        - Predict the outcomes for the test set and compute the \\( R^2 \\) score.\n",
        "\n",
        "4. **Model Selection:**\n",
        "    * Identify the regression coefficients that give the highest \\( R^2 \\) score on the validation set.\n",
        "    * This set of coefficients represents the best model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06327708",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06327708",
        "outputId": "a892f4d8-333e-42f2-8fbd-b8460bc131ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2 Score on Test Set: 0.8136488565357357\n",
            "Best R2 Score on Validation Set: 0.8050844247916222\n",
            "Best Regression Coefficients: [[-0.21189417]\n",
            " [ 0.49591645]\n",
            " [ 0.33031092]\n",
            " [ 0.20804856]\n",
            " [ 0.02456285]\n",
            " [ 0.30067222]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score\n",
        "from random import randrange\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# Split the dataset into training and testing sets\n",
        "def custom_split(data, ratio):\n",
        "    train_set = pd.DataFrame()\n",
        "    desired_train_size = int(ratio * len(data))\n",
        "    data_copy = data.copy()\n",
        "\n",
        "    while len(train_set) < desired_train_size:\n",
        "        idx = randrange(len(data_copy))\n",
        "        train_set = train_set.append(data_copy.iloc[idx], ignore_index=True)\n",
        "        data_copy = data_copy.drop(index=idx).reset_index(drop=True)\n",
        "\n",
        "    return train_set, data_copy\n",
        "\n",
        "# Implementing gradient descent for regression\n",
        "def gradient_descent(X, Y, lr, num_iterations):\n",
        "    num_samples = Y.shape[0]\n",
        "    coefficients = np.zeros((X.shape[1], 1))\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        predictions = np.dot(X, coefficients)\n",
        "        error = predictions - Y\n",
        "        gradient = (1/num_samples) * np.dot(X.T, error)\n",
        "        coefficients -= lr * gradient\n",
        "\n",
        "    return coefficients\n",
        "\n",
        "data = pd.read_csv('./USA_Housing.csv')\n",
        "normalized_data = (data - data.min()) / (data.max() - data.min())\n",
        "\n",
        "train_val_set, test_set = custom_split(normalized_data, 0.7)\n",
        "train_set, val_set = custom_split(train_val_set, 0.8)\n",
        "\n",
        "features = ['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',\n",
        "            'Avg. Area Number of Bedrooms', 'Area Population']\n",
        "\n",
        "X_train = np.column_stack((np.ones(len(train_set)), train_set[features].values))\n",
        "y_train = train_set['Price'].values.reshape(-1, 1)\n",
        "X_val = np.column_stack((np.ones(len(val_set)), val_set[features].values))\n",
        "y_val = val_set['Price'].values.reshape(-1, 1)\n",
        "X_test = np.column_stack((np.ones(len(test_set)), test_set[features].values))\n",
        "y_test = test_set['Price'].values.reshape(-1, 1)\n",
        "\n",
        "best_r2_val = float('-inf')\n",
        "best_coefficients = None\n",
        "\n",
        "# Training the model using different learning rates\n",
        "learning_rates = [0.001, 0.01, 0.1, 1]\n",
        "\n",
        "for lr in learning_rates:\n",
        "    coeffs = gradient_descent(X_train, y_train, lr, 1000)\n",
        "    predictions_val = np.dot(X_val, coeffs)\n",
        "    r2_val = r2_score(y_val, predictions_val)\n",
        "\n",
        "    if r2_val > best_r2_val:\n",
        "        best_r2_val = r2_val\n",
        "        best_coefficients = coeffs\n",
        "\n",
        "# Evaluate the model on the test set using the best coefficients\n",
        "predictions_test = np.dot(X_test, best_coefficients)\n",
        "r2_test = r2_score(y_test, predictions_test)\n",
        "\n",
        "print(\"R2 Score on Test Set:\", r2_test)\n",
        "print(\"Best R2 Score on Validation Set:\", best_r2_val)\n",
        "print(\"Best Regression Coefficients:\", best_coefficients)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95610ba5",
      "metadata": {
        "id": "95610ba5"
      },
      "source": [
        "#### Q3. Pre-processing and Multiple Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4fe49f5",
      "metadata": {
        "id": "a4fe49f5"
      },
      "source": [
        "\n",
        "Predict car prices using linear regression, with and without dimensionality reduction through PCA, on the provided dataset.\n",
        "\n",
        "**Dataset link** https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. **Data Loading and Cleaning:**\n",
        "    * Load the dataset using the provided column names: [\"symboling\", \"normalized_losses\", ... , \"price\"]\n",
        "    * Replace all \"?\" entries with NaN values for consistency.\n",
        "\n",
        "2. **Data Imputation:**\n",
        "    * For the 'price' column, remove any rows containing NaN values.\n",
        "    * For all other columns, replace NaN values using central tendency imputation methods (like mean or median).\n",
        "\n",
        "3. **Data Encoding:**\n",
        "    * For the \"num_doors\" and \"num_cylinders\" columns, convert textual number representations to their numeric equivalents (e.g., \"two\" to 2).\n",
        "    * Apply dummy encoding for columns \"body_style\" and \"drive_wheels\".\n",
        "    * Use label encoding for “make”, “aspiration”, “engine_location”, and \"fuel_type\" columns.\n",
        "    * For the \"fuel_system\" column, assign a value of 1 if the entry contains the string \"pfi\", otherwise assign 0.\n",
        "    * For the \"engine_type\" column, assign a value of 1 if the entry contains the string \"ohc\", otherwise assign 0.\n",
        "\n",
        "4. **Feature Selection and Scaling:**\n",
        "    * Segregate the dataset into predictor variables (all columns excluding 'price') and the target variable ('price').\n",
        "    * Normalize the predictor variables to ensure all are on a similar scale.\n",
        "\n",
        "5. **Linear Regression - Training and Evaluation:**\n",
        "    * Partition the dataset: use 70% for training and 30% for testing.\n",
        "    * Train a linear regression model using the training set and evaluate its performance on the test set.\n",
        "\n",
        "6. **Dimensionality Reduction and Model Retraining:**\n",
        "    * Implement PCA (Principal Component Analysis) to reduce the dimensionality of the feature set.\n",
        "    * Retrain the linear regression model using 70% of the transformed data and test its performance on the remaining 30%.\n",
        "    * Compare the performance of the model trained with PCA-transformed data to the one trained on the original data to determine if there's an improvement in prediction accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35ac0075",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35ac0075",
        "outputId": "25668996-9bf9-43c1-e3c4-a8d86973027c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE with original data: 0.07057604698276639\n",
            "MAE with PCA data: 0.07352461241675472\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Map to convert word numbers to actual numbers\n",
        "number_mapping = {\n",
        "    'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5,\n",
        "    'six': 6, 'seven': 7, 'eight': 8, 'nine': 9, 'twelve': 12\n",
        "}\n",
        "\n",
        "def word_to_num(word):\n",
        "    return number_mapping.get(word, word)\n",
        "\n",
        "def load_data():\n",
        "    df = pd.read_csv('./imports-85.data', na_values=\"?\")\n",
        "    df.columns = [\"symboling\", \"normalized_losses\",\"make\", \"fuel_type\", \"aspiration\",\"num_doors\", \"body_style\", \"drive_wheels\",\"engine_location\", \"wheel_base\", \"length\", \"width\", \"height\", \"curb_weight\",\"engine_type\", \"num_cylinders\", \"engine_size\", \"fuel_system\", \"bore\", \"stroke\",\"compression_ratio\", \"horsepower\", \"peak_rpm\", \"city_mpg\", \"highway_mpg\", \"price\"]\n",
        "\n",
        "    df.dropna(subset=['price'], inplace=True)\n",
        "    return df\n",
        "\n",
        "def encode_non_numeric_data(df):\n",
        "    # Convert word numbers to actual numbers\n",
        "    df['num_doors'] = df['num_doors'].apply(word_to_num)\n",
        "    df['num_cylinders'] = df['num_cylinders'].apply(word_to_num)\n",
        "\n",
        "    # One-hot encode\n",
        "    df = pd.get_dummies(df, columns=['body_style', 'drive_wheels'])\n",
        "\n",
        "    # Label encode\n",
        "    for col in [\"make\", \"aspiration\", \"engine_location\", \"fuel_type\"]:\n",
        "        df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "    # Binary encode based on presence of specific substrings\n",
        "    df['fuel_system'] = df['fuel_system'].apply(lambda x: 1 if 'pfi' in x else 0)\n",
        "    df['engine_type'] = df['engine_type'].apply(lambda x: 1 if 'ohc' in x else 0)\n",
        "\n",
        "    return df\n",
        "\n",
        "def impute_data(df):\n",
        "    imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "    df = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "    return df\n",
        "\n",
        "def train_and_evaluate(df):\n",
        "    x = df.drop(\"price\", axis=1)\n",
        "    y = df[[\"price\"]]\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    x = scaler.fit_transform(x)\n",
        "    y = scaler.fit_transform(y)\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=0.7, random_state=0)\n",
        "\n",
        "    model = LinearRegression().fit(X_train, Y_train)\n",
        "    Y_pred = model.predict(X_test)\n",
        "\n",
        "    mae = mean_absolute_error(Y_test, Y_pred)\n",
        "    return mae\n",
        "\n",
        "def main():\n",
        "    df = load_data()\n",
        "    df = impute_data(df)\n",
        "    df = encode_non_numeric_data(df)\n",
        "\n",
        "    # Evaluate model with original data\n",
        "    error_original = train_and_evaluate(df)\n",
        "    print(f\"MAE with original data: {error_original}\")\n",
        "\n",
        "    # Apply PCA\n",
        "    pca = PCA(n_components=20)\n",
        "    x_pca = pca.fit_transform(df.drop(\"price\", axis=1))\n",
        "    df_pca = pd.DataFrame(x_pca, columns=[f\"PC{i}\" for i in range(1, 21)])\n",
        "    df_pca[\"price\"] = df[\"price\"].values\n",
        "\n",
        "    # Evaluate model with PCA data\n",
        "    error_pca = train_and_evaluate(df_pca)\n",
        "    print(f\"MAE with PCA data: {error_pca}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P66q7jy338Ya",
      "metadata": {
        "id": "P66q7jy338Ya"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
